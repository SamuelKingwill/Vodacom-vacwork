{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN = 1 to drop NaN values in dataframe, else NaN = 0\n",
    "NaN = 1\n",
    "\n",
    "# correlated = 1 to drop correlated features in dataframe, else correlated = 0\n",
    "correlated = 1\n",
    "\n",
    "# correlation lower bound to drop features\n",
    "corr_bound = 0.91\n",
    "# minimum difference in IV to drop feature based on correlation bound\n",
    "iv_diff = 0.1 #Feat_sum.loc[:,\"Total_IV\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas\n",
    "import os.path\n",
    "import rpy2\n",
    "import scorecardpy as sc\n",
    "from scorecardpy import info_value \n",
    "import seaborn as seabornInstance \n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import RFE, RFECV, mutual_info_classif, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier as RFE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.expanduser( \"~/Documents/Vodacom/data/df_raw_final.pkl\" )\n",
    "\n",
    "df = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataframe after dropping all values with NaN\n",
    "dfclean = df.dropna( subset = ['target_label'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all strings \"yes\", \"no\", \"maybe\" with integers 0, 1, 2 \n",
    "dfclean = dfclean.replace( to_replace = \"yes\", value = 1 )\n",
    "dfclean = dfclean.replace( to_replace = \"no\", value = 0 )\n",
    "dfclean = dfclean.replace( to_replace = \"maybe\", value = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows with target_label = 2 (\"maybe\")\n",
    "dfclean = dfclean[ dfclean.target_label != 2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns that are targets \n",
    "for feature in dfclean :\n",
    "    \n",
    "    if (dfclean[feature].isna().sum() > 100) :\n",
    "        dfclean = dfclean.drop( columns = feature )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 'geometry' in dataframe causes unwanted errors.\n",
    "Initialize new dataframe without the 'geometry' column.\n",
    "Assumption that the 'geometry' column causes more issues than has relevance for model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column geometry due to string error with conversion\n",
    "dfclean_nogeo = dfclean.drop( columns = \"geometry\")\n",
    "\n",
    "# add randomized feature\n",
    "# dfclean_nogeo['randfeature'] = np.random.normal(size = dfclean_nogeo.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating lists of numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Feat_typ = {}\n",
    "Num_typ = {}\n",
    "\n",
    "num_list = []\n",
    "cat_list = []\n",
    "index_list = []\n",
    "\n",
    "j = 1\n",
    "\n",
    "# for loop to identify feature types\n",
    "\n",
    "for i, feature in enumerate( dfclean_nogeo ):\n",
    "    \n",
    "    if ( dfclean_nogeo[ feature ].dtype == 'float64' or dfclean_nogeo[ feature ].dtype == 'int64'):\n",
    "        \n",
    "        value_list = dfclean_nogeo[feature].values\n",
    "        length_values_list = []\n",
    "        \n",
    "# for loop identifying if all items in a given feature have the same number of characters\n",
    "# if they do, identify feature as a category\n",
    "        \n",
    "        for item in value_list:\n",
    "            \n",
    "            length_values_list.append( len( str( item ) ) )\n",
    "\n",
    "# CODE = true if number of characters in values of all features are the same            \n",
    "            \n",
    "        CODE = ( len( set( length_values_list ) ) == 1)\n",
    "        \n",
    "    if (dfclean_nogeo[feature].dtype == 'object' or CODE):     \n",
    "\n",
    "        Feat_typ[feature] = [i,\"cat\"]\n",
    "        index_list.append(feature)\n",
    "        if (feature != 'target_label') :\n",
    "            \n",
    "            cat_list.append(feature)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        Feat_typ[feature] = [i, \"num\"]\n",
    "        Num_typ[feature] = [j, \"num\"]\n",
    "        num_list.append(feature)\n",
    "        index_list.append(feature)\n",
    "        j = j + 1\n",
    "    \n",
    "    # adding the 'target_label' column to num_list    \n",
    "    \n",
    "    if feature == \"target_label\":\n",
    "\n",
    "        num_list.append( feature )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activates the use of R in the Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brute force fix for kernel dying on Mac, Error15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i dfclean_nogeo -i num_list -o cuts\n",
    "\n",
    "library(smbinning)\n",
    "library(foreach)\n",
    "library(doParallel)\n",
    "\n",
    "\n",
    "comb <- function(...){\n",
    "    mapply('cbind', ..., SIMPLIFY = FALSE)\n",
    "}\n",
    "\n",
    "no_cores <- detectCores()\n",
    "registerDoParallel(cores = 4)\n",
    "\n",
    "numg_list = c()\n",
    "cutsgood = c()\n",
    "\n",
    "Features <- names(dfclean_nogeo)[1:ncol(dfclean_nogeo)]\n",
    "\n",
    "i = 1\n",
    "j = 1\n",
    "\n",
    "ptime <- system.time({\n",
    "\n",
    " cuts = foreach( col = Features, .combine = comb, .packages = 'smbinning')  %dopar%  {\n",
    "\n",
    "    sbin = smbinning( dfclean_nogeo, 'target_label', col )\n",
    "            \n",
    "       if ( !is.na( sbin[ 'bands' ] ) ) {\n",
    "        \n",
    "            cutsgood[[i]] <- sbin$bands[ -length( sbin$bands ) ]\n",
    "            numg_list[i] = col\n",
    "            i = i + 1\n",
    "           \n",
    "        } \n",
    "     \n",
    "        # adding target_label to numg_list\n",
    "        if ( col == \"target_label\" ) {\n",
    "\n",
    "            numg_list[i] = col\n",
    "\n",
    "        }\n",
    "     \n",
    "     return(list(numg_list, cutsgood))\n",
    "     \n",
    "    }\n",
    "    \n",
    "})[3]\n",
    "print(paste(\"Time taken for smbinning =\", ptime, \"seconds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list for numg_list from an R vector\n",
    "cuts = list(cuts)\n",
    "\n",
    "# creating dictionary of value cutsgood\n",
    "cutsg_dic = dict( zip( cuts[0], cuts[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The woebin function creates IV tables for all numerical features in dataframe. It uses smbinning's cut values for the bins for all numerical features within cutsg_dic, all other numerical features have bins cut by the woebin function. \n",
    "This is due to the assumption that smbinning's cut values for the bins will yielf a maximized IV while, woebinning's cut values will not be maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns that are targets or categories\n",
    "\n",
    "dfclean_nogeo = dfclean_nogeo.drop( columns = cat_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information values of all numerical features\n",
    "num_iv = sc.woebin( dfclean_nogeo.loc[ :, num_list ],\n",
    "                   y = \"target_label\", breaks_list = cutsg_dic, positive = 0, bin_num_limit = 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe summarizing features\n",
    "Feat_sum = pd.DataFrame( dfclean_nogeo.dtypes )\n",
    "\n",
    "i = 0\n",
    "\n",
    "for feature in dfclean_nogeo:\n",
    "    \n",
    "    Feat_sum.loc[ feature,\"Unique\" ] = dfclean_nogeo[ feature ].nunique()\n",
    "    Feat_sum.loc[ feature, \"NaNs\" ] = dfclean_nogeo[ feature ].isna().sum()\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commented out code that follows is for the use of all and categorical features. Decision was made to use only Numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_iv = sc.woebin( dfclean_nogeo.loc[ :, index_list ],\n",
    "#                    y = \"target_label\", breaks_list = cutsg_dic, positive = 0, bin_num_limit = 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV values of all categorical features in dataframe\n",
    "#cat_iv = sc.info_value.iv( dfclean_nogeo.loc[ :, cat_list ], y = \"target_label\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # changing the headings of the dataframe cat_iv\n",
    "# cat_iv = cat_iv.rename( columns = { 'variable':'', 'info_value': 'Total_IV' } )\n",
    "\n",
    "# cat_iv = cat_iv.set_index('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding iv values of numerical features to summary dataframe\n",
    "# for feature in dfclean_nogeo.loc[ :, num_list ]:\n",
    "    \n",
    "#     if ( feature != 'target_label') :\n",
    "        \n",
    "#         Feat_sum.loc[ feature,'Total_IV' ] = num_iv[ feature ].total_iv[ 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding iv values of categorical features to summary datframe\n",
    "# for feature in dfclean_nogeo.loc[ :, cat_list ]:\n",
    "    \n",
    "#     if ( feature != 'target_label' ) :\n",
    "        \n",
    "#         Feat_sum.loc[ feature,'Total_IV' ] = cat_iv.loc[ feature, 'Total_IV' ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding iv values of numerical features to summary dataframe\n",
    "for feature in dfclean_nogeo.loc[ :, num_list ]:\n",
    "    \n",
    "    if ( feature != 'target_label' ) :\n",
    "        \n",
    "        Feat_sum.loc[ feature,'Total_IV' ] = num_iv[ feature ].total_iv[ 1 ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('Feat_sum.data', rb) as filehandle :\n",
    "# Feat_sum.to_csv('Feat_sum.csv')\n",
    "# df_final.to_csv('df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feat_sum.sort_values('Total_IV', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation matrix\n",
    "cor = dfclean_nogeo.corr()\n",
    "\n",
    "# cor.style.background_gradient(cmap = 'coolwarm')\n",
    "\n",
    "# joining Feat_sum matrix and correlation matrix\n",
    "cor = cor.join(Feat_sum.Total_IV, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_feat = []\n",
    "check_feat = []\n",
    "\n",
    "for feature, correl in cor.iteritems():\n",
    "    \n",
    "    #for each feature\n",
    "    for i in np.arange(len(correl.values)):\n",
    "    \n",
    "    # run through each row of cor\n",
    "        \n",
    "        if (abs(correl[i]) > corr_bound and correl.index[i] != feature and feature != \"Total_IV\"):\n",
    "    #if the correlation is high and less than one and ignore total IV\n",
    "            \n",
    "            if (cor.loc[correl.index[i], \"Total_IV\"] - cor.loc[feature, \"Total_IV\"] > iv_diff):\n",
    "    #if the IV of the row feature is more than 0.1 greater than the IV of the column feature, add the comun to the remove list\n",
    "#                 print(\"row \",correl.index[i],\"   is bigger than column     \", feature)\n",
    "                rem_feat.append(feature)\n",
    "            \n",
    "            elif(cor.loc[correl.index[i], \"Total_IV\"] - cor.loc[feature, \"Total_IV\"] > 0):\n",
    "    #if the features have very similar IVs, add them to the check list\n",
    "                check_feat.append([feature, correl.index[i]])\n",
    "\n",
    "rem_feat = list(dict.fromkeys(rem_feat))     #original order\n",
    "\n",
    "# rem_feat =  list(set(rem_feat))            #fastest order\n",
    "# rem_feat =  set(rem_feat)                  #alphabetical order\n",
    "# rem_feat =  rem_feat                       #with duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next kernel refers to the first kernel of the notebook. It is used so simplify the experimental process to identify optimal feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# drop all rows containing NaN values \n",
    "if (NaN == 1):\n",
    "    \n",
    "    df_final_noNaN = dfclean_nogeo.dropna()\n",
    "    \n",
    "else:\n",
    "        \n",
    "    df_final_noNaN = dfclean_nogeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns based off of correlation comparison\n",
    "if (correlated == 1) :\n",
    "    \n",
    "    df_final_noNaN = df_final_noNaN.drop(columns = rem_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_woe = sc.woebin( dfclean_nogeo.loc[ :, cat_list ], y = \"target_label\", bin_num_limit = 20 )\n",
    "#df_final2 = sc.woebin_ply(df_final, cat_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data table to WOE values\n",
    "\n",
    "#df_final = sc.woebin_ply(df_final_noNaN, num_iv)\n",
    "df_final = df_final_noNaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV list maximization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, feat in enumerate(Feat_sum.iterrows()):\n",
    "#     Feat_sum = Feat_sum.rename(index = {feat[0]: feat[0] + \"_woe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feat_sum = Feat_sum.loc[df_final.columns]\n",
    "Feat_sum = Feat_sum.sort_values(\"Total_IV\", ascending = False)\n",
    "Feat_sum.drop(Feat_sum.tail(1).index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_feat_acc_iv = []\n",
    "score2 = 0\n",
    "nom_feat = 1\n",
    "for no_feat in np.arange(len(Feat_sum)):\n",
    "    \n",
    "    used_feat = list(Feat_sum[0:no_feat+1].index)\n",
    "    df_final_X = df_final.loc[:,used_feat]\n",
    "    df_final_Y = pd.DataFrame(df_final.target_label)\n",
    "    X_train_iv, X_test_iv, y_train_iv, y_test_iv = train_test_split(df_final_X, df_final_Y,\n",
    "                                                       test_size = 0.3,stratify = df_final_Y,\n",
    "                                                       random_state = 40)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_iv, y_train_iv)\n",
    "    y_pred_iv = logreg.predict(X_test_iv)\n",
    "    score1 = logreg.score(X_test_iv, y_test_iv)\n",
    "    if (score1 > score2) :\n",
    "        score2 = score1\n",
    "        nom_feat = nom_feat + 1\n",
    "        best_feat = list(Feat_sum[0:nom_feat].index)\n",
    "        no_feat_acc_iv.append([logreg.score(X_test_iv, y_test_iv), nom_feat, best_feat])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.889589905362776, 56 when no correlation\n",
    "# 0.886435331230284, 77 when correlation dropped at 0.92\n",
    "# 0.8911671924290221, 52 when correlation dropped at 0.9\n",
    "# 0.8801261829652997, 43 when correlation dropped at 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.889589905362776, 11  when no correlation\n",
    "# 0.8911671924290221, 10  when corr dropped at 0.91\n",
    "# 0.886435331230284, 9  when corr dropped at 0.92\n",
    "# 0.8927444794952681, 10  when corr dropped at 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(no_feat_acc_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC(kernel='linear', C=1).fit(X_train_iv, y_train_iv)\n",
    "# clf.score(X_test_iv, y_test_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# clf = SVC(kernel='linear', C=1)\n",
    "# scores = cross_val_score(clf, X_test_iv, y_test_iv, cv=5)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOE and not WOE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_x_woe = df_final.drop([\"target_label\"], axis = 1)\n",
    "df_final_y_woe = pd.DataFrame(df_final.target_label)\n",
    "\n",
    "df_final_x = df_final_noNaN.drop([\"target_label\"], axis = 1)\n",
    "df_final_y = pd.DataFrame(df_final_noNaN.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_woe, X_test_woe, y_train_woe, y_test_woe = train_test_split(df_final_x_woe, df_final_y_woe, \n",
    "                                                test_size = 0.3, stratify = df_final_y_woe,\n",
    "                                                random_state = 40)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final_x, df_final_y, \n",
    "                                                test_size = 0.3, stratify = df_final_y,\n",
    "                                                random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_woe = LogisticRegression()  \n",
    "regressor_woe.fit(X_train_woe, y_train_woe)\n",
    "  \n",
    "y_pred_woe = regressor_woe.predict(X_test_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LogisticRegression()  \n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feature_woe = mutual_info_classif(X_train_woe, y_train_woe)\n",
    "\n",
    "select_feature = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df_woe = pd.DataFrame({'Features': list(X_train_woe.columns),\n",
    "                                     'Scores': select_feature_woe})\n",
    "selected_features_df_woe = selected_features_df_woe.sort_values(by = 'Scores', ascending = False)\n",
    "\n",
    "selected_features_df = pd.DataFrame({'Features': list(X_train.columns),\n",
    "                                     'Scores': select_feature})\n",
    "selected_features_df = selected_features_df.sort_values(by = 'Scores', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_feat_acc_mi = []\n",
    "score2 = 0\n",
    "nom_feat = 1\n",
    "for no_feat in np.arange(len(selected_features_df_woe)):\n",
    "    \n",
    "    used_feat = list(selected_features_df_woe.iloc[0:no_feat+1,0])\n",
    "    df_final_X = df_final.loc[:,used_feat]\n",
    "    df_final_Y = pd.DataFrame(df_final.target_label)\n",
    "    X_train_mi, X_test_mi, y_train_mi, y_test_mi = train_test_split(df_final_X, df_final_Y,\n",
    "                                                       test_size = 0.3,stratify = df_final_Y,\n",
    "                                                       random_state = 40)\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_mi, y_train_mi)\n",
    "    y_pred = logreg.predict(X_test_mi)\n",
    "    score1 = logreg.score(X_test_mi, y_test)\n",
    "    if (score1 > score2) :\n",
    "        score2 = score1\n",
    "        nom_feat = nom_feat + 1\n",
    "        best_feat = list(selected_features_df_woe[0:nom_feat].Features)\n",
    "        no_feat_acc_mi.append([logreg.score(X_test_mi, y_test_mi), nom_feat, best_feat])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.88801261829653, 58 with nothing removed\n",
    "# 0.891167, 62 when correlation dropped at 0.92\n",
    "# 0.889589905362776, 49 when correlation dropped at 0.9\n",
    "# 0.8801261829652997, 46 when correlation dropped at 0.8\n",
    "# 0.8943217665615142, 13 when correlation dropped at 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8927444794952681, 17 withh correlation not dropped\n",
    "# 0.8832807570977917, 11 withh corr dropped at 0.91\n",
    "# 0.889589905362776, 8 withh corr dropped at 0.92\n",
    "# 0.8911671924290221, 10 withh corr dropped at 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(no_feat_acc_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_woe = metrics.confusion_matrix(y_test_woe, y_pred_woe)\n",
    "print(confusion_matrix_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_woe = regressor_woe.score(X_test_woe, y_test_woe)\n",
    "print(score_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = regressor.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_woe, y_pred_woe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_roc_auc_woe = roc_auc_score(y_test_woe, regressor_woe.predict(X_test_woe))\n",
    "# fpr, tpr, thresholds = roc_curve(y_test_woe, regressor_woe.predict_proba(X_test_woe)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc_woe)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('Log_ROC')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_roc_auc = roc_auc_score(y_test, regressor.predict(X_test))\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, regressor.predict_proba(X_test)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('Log_ROC')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfe_woe = RFE( random_state = 101 )\n",
    "svc_woe = SVC(kernel=\"linear\")\n",
    "rfecv_woe = RFECV( estimator = svc_woe, step = 1, cv = StratifiedKFold(10), scoring = 'accuracy', n_jobs = 4 )\n",
    "rfecv_woe.fit( X_test_woe, y_test_woe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE( random_state = 101 )\n",
    "#svc = SVC(kernel=\"linear\")\n",
    "rfecv = RFECV( estimator = rfe, step = 1, cv = StratifiedKFold(10), scoring = 'accuracy', n_jobs = 4 )\n",
    "rfecv.fit( X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOE transformation\n",
    "\n",
    "# 0.8753943217665615, 30 with correlation not dropped\n",
    "# 0.8785488958990536, 37 with corr dropped at 0.91\n",
    "# 0.886435331230284, 30 with corr dropped at 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (correlated == 1):\n",
    "    print('Highly correlated features removed')\n",
    "print('Optimal number of features with woe: {}'.format(rfecv_woe.n_features_))\n",
    "print('Out of total: ', selected_features_df_woe.Features.nunique(), ' features')\n",
    "print('Score', ' = ' ,score_woe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features_df_woe = pd.DataFrame({'Features': list(X_train_woe.columns),\n",
    "                                     'Scores': rfecv_woe.support_})\n",
    "#optimal_features_df_woe.loc[optimal_features_df_woe['Scores'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no WOE transformation\n",
    "\n",
    "# 0.7886435331230284, 39 with correlation not dropped\n",
    "# 0.7854889589905363, 17 with corr dropped at 0.91\n",
    "# 0.8012618296529969, 69 with corr dropped at 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (correlated == 1):\n",
    "    print('Highly correlated features removed')\n",
    "print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "print('Out of total: ', selected_features_df.Features.nunique(), ' features')\n",
    "print('Score', ' = ' ,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_features_df = pd.DataFrame({'Features': list(X_train.columns),\n",
    "                                     'Scores': rfecv.support_})\n",
    "#optimal_features_df.loc[optimal_features_df['Scores'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 9))\n",
    "# plt.title('Recursive Feature Elimination WOE with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "# plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "# plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "# plt.plot(range(1, len(rfecv_woe.grid_scores_) + 1), rfecv_woe.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16, 9))\n",
    "# plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "# plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "# plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(rfecv.support_ == False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal number of features: {}'.format(rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV\n",
    "max(no_feat_acc_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information\n",
    "max(no_feat_acc_mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
